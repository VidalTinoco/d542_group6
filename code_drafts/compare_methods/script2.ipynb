{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial data rows: 353\n",
      "After exploding 'ChatgptSharing': 417 rows\n",
      "Unique conversation IDs: 406\n",
      "After exploding 'Conversations': 1780 rows\n",
      "Language detection applied. Null languages: 39\n",
      "English conversations: 1455 rows, 313 unique IDs\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas import json_normalize\n",
    "import numpy as np\n",
    "from langdetect import detect, LangDetectException\n",
    "from langdetect.detector_factory import DetectorFactory\n",
    "\n",
    "# Set a fixed seed for reproducibility\n",
    "DetectorFactory.seed = 108\n",
    "\n",
    "def detect_language(prompt):\n",
    "    \"\"\"Detects the language of a given text, handling exceptions.\"\"\"\n",
    "    try:\n",
    "        return detect(str(prompt)) if isinstance(prompt, str) else None\n",
    "    except LangDetectException:\n",
    "        return None\n",
    "\n",
    "\n",
    "def load_and_normalize_json(file_path):\n",
    "    \"\"\"Loads a JSON file and normalizes its 'Sources' data.\"\"\"\n",
    "    df = pd.read_json(file_path)\n",
    "    df = json_normalize(df['Sources'])\n",
    "    print(f\"Initial data rows: {len(df)}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_and_transform_data(df):\n",
    "    \"\"\"Cleans and renames columns in the dataset.\"\"\"\n",
    "    columns_to_drop = ['Type', 'Author']\n",
    "    df.drop(columns=columns_to_drop, errors='ignore', inplace=True)\n",
    "    \n",
    "    df.rename(columns={\n",
    "        \"Title\": \"issueTitle\",\n",
    "        \"URL\": \"sourceURL\",\n",
    "        \"Body\": \"issueDesc\",\n",
    "        \"Number\": \"numPrompts\"\n",
    "    }, inplace=True)\n",
    "    \n",
    "    df['ChatgptSharing'] = df['ChatgptSharing'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "    return df\n",
    "\n",
    "\n",
    "def normalize_and_explode_chatgpt(df):\n",
    "    \"\"\"Explodes the 'ChatgptSharing' column and propagates relevant columns.\"\"\"\n",
    "    chatgpt_sharing = json_normalize(df['ChatgptSharing'].explode())\n",
    "    print(f\"After exploding 'ChatgptSharing': {len(chatgpt_sharing)} rows\")\n",
    "    \n",
    "    if 'Title' in chatgpt_sharing.columns and 'Mention.MentionedURL' in chatgpt_sharing.columns:\n",
    "        chatgpt_sharing['conversation_id'] = chatgpt_sharing['Title'].astype(str) + '_' + chatgpt_sharing['Mention.MentionedURL'].astype(str)\n",
    "    else:\n",
    "        chatgpt_sharing['conversation_id'] = \"UNKNOWN\"\n",
    "        \n",
    "    print(f\"Unique conversation IDs: {chatgpt_sharing['conversation_id'].nunique()}\")\n",
    "    \n",
    "    # Propagate columns from the parent DataFrame\n",
    "    columns_to_propagate = df.columns.difference(['ChatgptSharing'])\n",
    "    for col in columns_to_propagate:\n",
    "        chatgpt_sharing[col] = df[col].repeat(df['ChatgptSharing'].apply(len)).reset_index(drop=True)\n",
    "    \n",
    "    return chatgpt_sharing\n",
    "\n",
    "\n",
    "def clean_chatgpt_sharing(chatgpt_sharing):\n",
    "    \"\"\"Cleans and renames columns in the exploded ChatGPT sharing data.\"\"\"\n",
    "    columns_to_drop = [\n",
    "        'Status', 'DateOfConversation', 'DateOfAccess', 'NumberOfPrompts', 'TokensOfPrompts', \n",
    "        'TokensOfAnswers', 'Model', 'HTMLContent', 'URL', 'Mention.MentionedURL', \n",
    "        'Mention.MentionedAuthor'\n",
    "    ]\n",
    "    chatgpt_sharing.drop(columns=columns_to_drop, errors='ignore', inplace=True)\n",
    "    \n",
    "    chatgpt_sharing.rename(columns={\n",
    "        'Title': 'conversationTitle',\n",
    "        'Mention.MentionedProperty': 'mentionProperty',\n",
    "        'Mention.MentionedText': 'mentionText'\n",
    "    }, inplace=True)\n",
    "    \n",
    "    chatgpt_sharing['Conversations'] = chatgpt_sharing['Conversations'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "    return chatgpt_sharing\n",
    "\n",
    "\n",
    "def normalize_and_explode_conversations(chatgpt_sharing):\n",
    "    \"\"\"Explodes the 'Conversations' column and propagates relevant columns.\"\"\"\n",
    "    conversations = json_normalize(chatgpt_sharing['Conversations'].explode())\n",
    "    print(f\"After exploding 'Conversations': {len(conversations)} rows\")\n",
    "    \n",
    "    if 'conversation_id' not in conversations.columns:\n",
    "        conversations['conversation_id'] = chatgpt_sharing['conversation_id'].repeat(chatgpt_sharing['Conversations'].apply(len)).reset_index(drop=True)\n",
    "\n",
    "    columns_to_propagate = chatgpt_sharing.columns.difference(['Conversations'])\n",
    "    for col in columns_to_propagate:\n",
    "        conversations[col] = chatgpt_sharing[col].repeat(chatgpt_sharing['Conversations'].apply(len)).reset_index(drop=True)\n",
    "    \n",
    "    return conversations\n",
    "\n",
    "\n",
    "def filter_english_conversations(conversations):\n",
    "    \"\"\"Filters conversations to keep only those in English.\"\"\"\n",
    "    conversations['Detected_Language'] = conversations['Prompt'].apply(detect_language)\n",
    "    print(f\"Language detection applied. Null languages: {conversations['Detected_Language'].isnull().sum()}\")\n",
    "    \n",
    "    if 'conversation_id' in conversations.columns:\n",
    "        mode_languages = conversations.groupby('conversation_id')['Detected_Language'].agg(lambda x: x.mode()[0] if not x.mode().empty else None)\n",
    "        english_conversations = mode_languages[mode_languages == 'en'].index.tolist()\n",
    "        conversations = conversations[conversations['conversation_id'].isin(english_conversations)]\n",
    "        print(f\"English conversations: {len(conversations)} rows, {conversations['conversation_id'].nunique()} unique IDs\")\n",
    "    else:\n",
    "        print(\"Error: 'conversation_id' is missing from conversations dataset.\")\n",
    "    \n",
    "    return conversations\n",
    "\n",
    "\n",
    "def process_json(file_path):\n",
    "    \"\"\"Executes the entire processing pipeline for a given JSON file.\"\"\"\n",
    "    df = load_and_normalize_json(file_path)\n",
    "    df = clean_and_transform_data(df)\n",
    "    chatgpt_sharing = normalize_and_explode_chatgpt(df)\n",
    "    chatgpt_sharing = clean_chatgpt_sharing(chatgpt_sharing)\n",
    "    conversations = normalize_and_explode_conversations(chatgpt_sharing)\n",
    "    conversations = filter_english_conversations(conversations)\n",
    "    \n",
    "    return conversations\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "file_path = '../../data/snapshot_20230831/20230831_061759_issue_sharings.json'\n",
    "processed_conversations = process_json(file_path)\n",
    "\n",
    "# Now you can use `processed_conversations` for further analysis or save it as a CSV\n",
    "# processed_conversations.to_csv('cleaned_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conversation_id\n",
      "AHC Treatment Compounds_https://github.com/NCATSTranslator/Feedback/issues/198#issuecomment-1570587853                   6\n",
      "AHC Treatment Compounds List_https://github.com/NCATSTranslator/Feedback/issues/198#issuecomment-1570587853              3\n",
      "Minority Language Data Site_https://github.com/sillsdev/languageforge-lexbox/issues/60#issuecomment-1665792771           2\n",
      "Minority Language Data Suggestions_https://github.com/sillsdev/languageforge-lexbox/issues/60#issuecomment-1669728877    2\n",
      "Minority Language Data Site_https://github.com/sillsdev/languageforge-lexbox/issues/60#issuecomment-1669728877           2\n",
      "                                                                                                                        ..\n",
      "Map NumPy to ctypes_https://github.com/lcompilers/lpython/issues/2276#issuecomment-1677948059                            1\n",
      "Map NumPy to ctypes_https://github.com/lcompilers/lpython/issues/2276#issuecomment-1677912426                            1\n",
      "React Clipboard API_https://github.com/neilenns/vatsim-plan-verifier/issues/149                                          1\n",
      "GitHub Social Enhancer_https://github.com/Yukaii/open-source-ideas/issues/67#issuecomment-1596067072                     1\n",
      "TypeScript: No Checked Exceptions_https://github.com/microsoft/TypeScript/issues/13219#issuecomment-1693733605           1\n",
      "Name: count, Length: 406, dtype: int64\n",
      "0\n",
      "Detected_Language\n",
      "en       1266\n",
      "ja         39\n",
      "zh-cn      23\n",
      "de         19\n",
      "ko         13\n",
      "fr         13\n",
      "nl          8\n",
      "no          8\n",
      "it          8\n",
      "pt          6\n",
      "ca          5\n",
      "es          4\n",
      "cy          3\n",
      "vi          3\n",
      "ro          2\n",
      "et          2\n",
      "sw          2\n",
      "af          2\n",
      "sk          1\n",
      "fi          1\n",
      "pl          1\n",
      "tr          1\n",
      "sv          1\n",
      "so          1\n",
      "da          1\n",
      "Name: count, dtype: int64\n",
      "conversation_id\n",
      "4. Preloaded Ranking Style_https://github.com/Bgorson/giftology/issues/130                                        en\n",
      "AES vs SHA-256_https://github.com/The-Poolz/Acl.Net.Core/issues/20                                                ja\n",
      "AHC Treatment Compounds List_https://github.com/NCATSTranslator/Feedback/issues/198#issuecomment-1570587853       en\n",
      "AHC Treatment Compounds_https://github.com/NCATSTranslator/Feedback/issues/198#issuecomment-1570587853            en\n",
      "AHC Treatment: Drug Options_https://github.com/NCATSTranslator/Feedback/issues/198#issuecomment-1570536126        en\n",
      "                                                                                                                  ..\n",
      "属性の削除JavaScript_https://github.com/tyamap/form-sales-helper/issues/3#issuecomment-1582371740                      en\n",
      "提高上傳限制 (Increase Upload Limit)_https://github.com/numbersprotocol/capture-lite/issues/2881                        en\n",
      "论文_https://github.com/bcmi/Foreground-Object-Search-Dataset-FOSD/issues/2                                         en\n",
      "게시판 웹앱 중복 저장 해결 (4 words)_https://github.com/zhyunk/zb-mission-store-reserve/issues/49#issuecomment-1663557892    en\n",
      "프론트엔드 기술 스택_https://github.com/TimCookie/CookieJar/issues/10                                                      ko\n",
      "Name: Detected_Language, Length: 369, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(chatgpt_sharing['conversation_id'].value_counts())  \n",
    "print(conversations['Prompt'].isnull().sum())  \n",
    "print(conversations['Detected_Language'].value_counts())  \n",
    "print(mode_languages)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected_Language\n",
      "en       315\n",
      "ja        26\n",
      "ko        13\n",
      "ca         3\n",
      "de         3\n",
      "it         2\n",
      "zh-cn      2\n",
      "so         1\n",
      "et         1\n",
      "nl         1\n",
      "vi         1\n",
      "cs         1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#print(chatgpt_sharing['Mention.MentionedURL'].isnull().sum())\n",
    "print(mode_languages.value_counts())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
