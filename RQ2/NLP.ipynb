{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e1941d18-b851-4f11-820c-5788de6e3d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8d00077d-b5eb-4b26-b2d8-7a04c4b5b72a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('NL_code_difference_clean.csv', na_values=['', 'nan'])\n",
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7a9035d2-2992-43d4-a5be-9e0a0681f3c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# n = 30\n",
    "# for idx, row in enumerate(df['difference'].iloc[n:n+10]):\n",
    "#     print(idx)\n",
    "#     print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "000fbb04-ba5d-4662-813d-5ec7781f6572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 915 entries, 0 to 1094\n",
      "Data columns (total 21 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   Prompt             915 non-null    object\n",
      " 1   Answer             915 non-null    object\n",
      " 2   ListOfCode         915 non-null    object\n",
      " 3   conversation_id    915 non-null    object\n",
      " 4   ClosedAt           346 non-null    object\n",
      " 5   CreatedAt          915 non-null    object\n",
      " 6   Number             915 non-null    int64 \n",
      " 7   RepoLanguage       829 non-null    object\n",
      " 8   RepoName           915 non-null    object\n",
      " 9   State              915 non-null    object\n",
      " 10  UpdatedAt          915 non-null    object\n",
      " 11  conversationTitle  912 non-null    object\n",
      " 12  issueDesc          893 non-null    object\n",
      " 13  issueTitle         915 non-null    object\n",
      " 14  mentionProperty    915 non-null    object\n",
      " 15  mentionText        915 non-null    object\n",
      " 16  numPrompts         915 non-null    int64 \n",
      " 17  sourceURL          915 non-null    object\n",
      " 18  Detected_Language  915 non-null    object\n",
      " 19  extractedCode      915 non-null    object\n",
      " 20  difference         915 non-null    object\n",
      "dtypes: int64(2), object(19)\n",
      "memory usage: 157.3+ KB\n"
     ]
    }
   ],
   "source": [
    "# Drop rows with duplicate 'difference' but keep the first occurrence\n",
    "df = df.drop_duplicates(subset=['Prompt'], keep='first')\n",
    "\n",
    "# Check for NaN or empty strings in the original 'Prompt' column\n",
    "df['difference'] = df['difference'].str.replace(r'\\n+', ' ', regex=True).str.strip()\n",
    "\n",
    "# print(df['difference'].isna().sum())  # Check for NaNs\n",
    "# print(df['difference'].isnull().sum())  # Count of None values\n",
    "# print((df['difference'] == '').sum())  # Check for empty strings\n",
    "\n",
    "# Drop rows where 'difference' is an empty string\n",
    "df = df[df['difference'].notna() & (df['difference'] != '')]\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c8d66e70-dd52-4747-9b3c-486de68724b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 260 entries, 0 to 259\n",
      "Data columns (total 4 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   conversation_id  260 non-null    object\n",
      " 1   difference       260 non-null    object\n",
      " 2   State            260 non-null    object\n",
      " 3   numPrompts       260 non-null    int64 \n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 8.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df_merged = df.groupby('conversation_id').agg({\n",
    "    'difference': lambda x: ' '.join(x.dropna().astype(str)) if not x.dropna().empty else None,  # Avoid empty groups\n",
    "    'State': 'first',    # Retain the first value of State (assumed to be the same for all prompts in a conversation)\n",
    "    'numPrompts': 'first',  # Retain the first value of numPrompts\n",
    "}).reset_index()\n",
    "\n",
    "df_merged.info()\n",
    "df = df_merged # CHANGE IF NEEDED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6bb23e30-a4da-4f4b-9e5f-f80f2c00988b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from sklearn.cluster import KMeans\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# # Download required NLTK data\n",
    "# # nltk.download('stopwords')\n",
    "# # nltk.download('punkt')\n",
    "# # nltk.download('punkt_tab')\n",
    "\n",
    "# def create_custom_stopwords():\n",
    "#     \"\"\"Create custom stopwords list that preserves prompt structure words.\"\"\"\n",
    "#     stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "#     # Words to keep for capturing prompt structure\n",
    "#     structure_words = {\n",
    "#         # Question words\n",
    "#         'what', 'how', 'why', 'when', 'where', 'which',\n",
    "#         # Modal verbs\n",
    "#         'can', 'could', 'would', 'should',\n",
    "#         # Common request verbs\n",
    "#         'explain', 'show', 'tell', 'help', 'write', 'make', 'create',\n",
    "#         # To-be verbs\n",
    "#         'is', 'are', 'was', 'were',\n",
    "#         # Pronouns\n",
    "#         'i', 'you', 'it',\n",
    "#         # Additional structural words\n",
    "#         'do', 'does', 'did',\n",
    "#         'this', 'that',\n",
    "#         'mean', 'means',\n",
    "#         'need', 'want'\n",
    "#     }\n",
    "    \n",
    "#     # Convert the difference to a list\n",
    "#     return list(stop_words - structure_words)\n",
    "\n",
    "# def clean_text(text):\n",
    "#     \"\"\"Clean text by removing URLs, file paths, and code-specific terms.\"\"\"\n",
    "#     # Remove URLs\n",
    "#     text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    \n",
    "#     # Remove file paths\n",
    "#     text = re.sub(r'[a-zA-Z]:\\\\[\\\\\\S|*\\S]?.*|/\\S+', '', text)\n",
    "    \n",
    "#     # Remove common code terms\n",
    "#     code_terms = {\n",
    "#         'github', 'com', 'http', 'https', 'localhost', 'api',\n",
    "#         'json', 'xml', 'html', 'css', 'js', 'php', 'sql',\n",
    "#         'readme', 'md', 'txt', 'pdf', 'doc', 'docx'\n",
    "#     }\n",
    "    \n",
    "#     # Convert to lowercase and tokenize\n",
    "#     tokens = word_tokenize(text.lower())\n",
    "    \n",
    "#     # Remove code terms and non-alphabetic tokens\n",
    "#     tokens = [token.lemma_ for token in tokens \n",
    "#              if token.isalpha() and token.lower() not in code_terms]\n",
    "    \n",
    "#     return ' '.join(tokens)\n",
    "\n",
    "# def cluster_prompts(df, column_name, n_clusters=5):\n",
    "#     \"\"\"\n",
    "#     Cluster prompts using TF-IDF vectorization and K-means clustering.\n",
    "    \n",
    "#     Parameters:\n",
    "#     df : pandas DataFrame containing the prompts\n",
    "#     column_name : name of the column containing the prompts\n",
    "#     n_clusters : number of clusters to create\n",
    "    \n",
    "#     Returns:\n",
    "#     DataFrame with clusters and top words, cluster labels\n",
    "#     \"\"\"\n",
    "#     # Clean the texts\n",
    "#     df['cleaned_text'] = df[column_name].apply(clean_text)\n",
    "    \n",
    "#     # Create custom stopwords\n",
    "#     custom_stops = create_custom_stopwords()\n",
    "    \n",
    "#     # Initialize TF-IDF vectorizer\n",
    "#     tfidf_vectorizer = TfidfVectorizer(\n",
    "#         ngram_range=(2, 4),  # Capture 2-4 word phrases\n",
    "#         max_features=500, # best was 200 ?\n",
    "#         min_df=0.01,  # Remove terms that appear in less than 1% of documents\n",
    "#         max_df=0.7,   # Remove terms that appear in more than 70% of documents\n",
    "#         stop_words=custom_stops\n",
    "#     )\n",
    "    \n",
    "#     # Fit and transform the prompts\n",
    "#     X_tfidf = tfidf_vectorizer.fit_transform(df['cleaned_text'])\n",
    "    \n",
    "#     # Perform clustering\n",
    "#     kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "#     cluster_labels = kmeans.fit_predict(X_tfidf)\n",
    "    \n",
    "#     # Get top words for each cluster\n",
    "#     feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "#     cluster_centers = kmeans.cluster_centers_\n",
    "    \n",
    "#     # Store top words for each cluster\n",
    "#     top_words_per_cluster = {}\n",
    "#     n_top_words = 10\n",
    "    \n",
    "#     for i in range(n_clusters):\n",
    "#         center_vector = cluster_centers[i]\n",
    "#         top_indices = center_vector.argsort()[-n_top_words:][::-1]\n",
    "#         top_words = [feature_names[idx] for idx in top_indices]\n",
    "#         top_words_per_cluster[i] = top_words\n",
    "    \n",
    "#     return top_words_per_cluster, cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "677b69a7-fcb3-491b-ba52-8da93efb7a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load SpaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def create_custom_stopwords():\n",
    "    \"\"\"Create custom stopwords list that preserves prompt structure words.\"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Words to keep for capturing prompt structure\n",
    "    structure_words = {\n",
    "        # Question words\n",
    "        'what', 'how', 'why', 'when', 'where', 'which',\n",
    "        # Modal verbs\n",
    "        'can', 'could', 'would', 'should',\n",
    "        # Common request verbs\n",
    "        'explain', 'show', 'tell', 'help', 'write', 'make', 'create',\n",
    "        # To-be verbs\n",
    "        'is', 'are', 'was', 'were',\n",
    "        # Pronouns\n",
    "        'i', 'you', 'it',\n",
    "        # Additional structural words\n",
    "        'do', 'does', 'did',\n",
    "        'this', 'that',\n",
    "        'mean', 'means',\n",
    "        'need', 'want'\n",
    "    }\n",
    "    \n",
    "    # Convert the difference to a list\n",
    "    return list(stop_words - structure_words)\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean text by removing URLs, file paths, and code-specific terms using SpaCy.\"\"\"\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    \n",
    "    # Remove file paths\n",
    "    text = re.sub(r'[a-zA-Z]:\\\\[\\\\\\S|*\\S]?.*|/\\S+', '', text)\n",
    "    \n",
    "    # Remove common code terms\n",
    "    code_terms = {\n",
    "        'github', 'com', 'http', 'https', 'localhost', 'api',\n",
    "        'json', 'xml', 'html', 'css', 'js', 'php', 'sql',\n",
    "        'readme', 'md', 'txt', 'pdf', 'doc', 'docx'\n",
    "    }\n",
    "\n",
    "    # Process the text with SpaCy\n",
    "    doc = nlp(text.lower())  # Convert to lowercase and process with SpaCy\n",
    "    \n",
    "    # Lemmatize and filter tokens\n",
    "    tokens = [\n",
    "        token.lemma_ for token in doc \n",
    "        if token.is_alpha and token.text not in code_terms\n",
    "    ]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def cluster_prompts(df, column_name, n_clusters=5):\n",
    "    \"\"\"Cluster prompts using TF-IDF vectorization and K-means clustering.\"\"\"\n",
    "    # Clean the texts\n",
    "    df['cleaned_text'] = df[column_name].apply(clean_text)\n",
    "    \n",
    "    # Create custom stopwords\n",
    "    custom_stops = create_custom_stopwords()\n",
    "    \n",
    "    # Initialize TF-IDF vectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "        ngram_range=(2, 4),  # Capture 2-4 word phrases\n",
    "        max_features=500,  # best was 200 ?\n",
    "        min_df=0.01,  # Remove terms that appear in less than 1% of documents\n",
    "        max_df=0.7,   # Remove terms that appear in more than 70% of documents\n",
    "        stop_words=custom_stops\n",
    "    )\n",
    "    \n",
    "    # Fit and transform the prompts\n",
    "    X_tfidf = tfidf_vectorizer.fit_transform(df['cleaned_text'])\n",
    "    print(X_tfidf)\n",
    "    \n",
    "    # Perform clustering\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(X_tfidf)\n",
    "    \n",
    "    # Get top words for each cluster\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    cluster_centers = kmeans.cluster_centers_\n",
    "    \n",
    "    # Store top words for each cluster\n",
    "    top_words_per_cluster = {}\n",
    "    n_top_words = 10\n",
    "    \n",
    "    for i in range(n_clusters):\n",
    "        center_vector = cluster_centers[i]\n",
    "        top_indices = center_vector.argsort()[-n_top_words:][::-1]\n",
    "        top_words = [feature_names[idx] for idx in top_indices]\n",
    "        top_words_per_cluster[i] = top_words\n",
    "    \n",
    "    return top_words_per_cluster, cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6bad0e7b-01f2-489e-812b-f6aa9a81f72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 1828 stored elements and shape (260, 381)>\n",
      "  Coords\tValues\n",
      "  (0, 163)\t0.7914170096670649\n",
      "  (0, 39)\t0.6112766287121084\n",
      "  (1, 294)\t0.47367112215044443\n",
      "  (1, 297)\t0.5310636760620872\n",
      "  (1, 161)\t0.5310636760620872\n",
      "  (1, 235)\t0.459976534157541\n",
      "  (2, 21)\t0.3550919576363947\n",
      "  (2, 372)\t0.6485226122974596\n",
      "  (2, 26)\t0.673296459934873\n",
      "  (3, 39)\t0.39061564472644295\n",
      "  (3, 161)\t0.5057282594459684\n",
      "  (3, 235)\t0.43803246670986035\n",
      "  (3, 125)\t0.4069308790781752\n",
      "  (3, 246)\t0.48393508185854345\n",
      "  (5, 39)\t0.28721130552577406\n",
      "  (5, 125)\t0.2992075474105599\n",
      "  (5, 99)\t0.3718511421591332\n",
      "  (5, 355)\t0.35582708610571295\n",
      "  (5, 203)\t0.35582708610571295\n",
      "  (5, 205)\t0.35582708610571295\n",
      "  (5, 109)\t0.2120997521797331\n",
      "  (5, 19)\t0.3718511421591332\n",
      "  (5, 306)\t0.35582708610571295\n",
      "  (6, 39)\t0.33175409161341496\n",
      "  (6, 319)\t0.3958881964506027\n",
      "  :\t:\n",
      "  (257, 115)\t1.0\n",
      "  (259, 39)\t0.17296388099897944\n",
      "  (259, 21)\t0.22602547751941668\n",
      "  (259, 99)\t0.22393553270477076\n",
      "  (259, 109)\t0.12773033508820653\n",
      "  (259, 352)\t0.21428555420106588\n",
      "  (259, 84)\t0.22393553270477076\n",
      "  (259, 299)\t0.22393553270477076\n",
      "  (259, 173)\t0.19973460781984526\n",
      "  (259, 253)\t0.16677539803874344\n",
      "  (259, 270)\t0.19395996158513523\n",
      "  (259, 247)\t0.16398439046549967\n",
      "  (259, 346)\t0.18430998308143032\n",
      "  (259, 194)\t0.22393553270477076\n",
      "  (259, 2)\t0.22393553270477076\n",
      "  (259, 277)\t0.22393553270477076\n",
      "  (259, 311)\t0.19395996158513523\n",
      "  (259, 358)\t0.1697590367002097\n",
      "  (259, 88)\t0.2064009476620839\n",
      "  (259, 33)\t0.21428555420106588\n",
      "  (259, 365)\t0.21428555420106588\n",
      "  (259, 34)\t0.22393553270477076\n",
      "  (259, 24)\t0.21428555420106588\n",
      "  (259, 58)\t0.2064009476620839\n",
      "  (259, 305)\t0.22393553270477076\n",
      "\n",
      "Cluster 0 top words: can you, how can, how do, do it, it do, do this, what do, you can, what would, can get\n",
      "\n",
      "Cluster 1 top words: do this, this error, do you, how do, this result, step step, do want, could you, what do, run it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/janeshen/.pyenv/versions/3.11.10/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['it', 'should', 'that', 'you'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "cluster\n",
       "1    182\n",
       "0     78\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform clustering\n",
    "top_words, labels = cluster_prompts(df, 'difference', 2)\n",
    "\n",
    "# Print results\n",
    "for cluster_id, words in top_words.items():\n",
    "    print(f\"\\nCluster {cluster_id} top words:\", ', '.join(words))\n",
    "\n",
    "# Add cluster labels to DataFrame\n",
    "df['cluster'] = labels\n",
    "\n",
    "# show distribution of cluster labels\n",
    "df['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "864a2021-67be-4bb2-b903-dbb14b5349c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1w/y2cbr0js165g3w970n3hbwqh0000gn/T/ipykernel_3979/1710834374.py:21: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x='State', y='Count', data=cluster_data, ax=axes[i], palette='Set2')\n",
      "/var/folders/1w/y2cbr0js165g3w970n3hbwqh0000gn/T/ipykernel_3979/1710834374.py:21: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x='State', y='Count', data=cluster_data, ax=axes[i], palette='Set2')\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Aggregate data: count occurrences of 'State' per 'cluster'\n",
    "agg_data = df.groupby(['cluster', 'State']).size().reset_index(name='Count')\n",
    "\n",
    "# Set up the plot\n",
    "clusters = agg_data['cluster'].unique()\n",
    "num_clusters = len(clusters)\n",
    "\n",
    "# Create a grid of subplots based on the number of clusters\n",
    "fig, axes = plt.subplots(1, num_clusters, figsize=(num_clusters * 5, 5), sharey=True)\n",
    "\n",
    "# If there is only one cluster, axes will not be an array, so we need to handle that\n",
    "if num_clusters == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "# Plot each cluster\n",
    "for i, cluster in enumerate(clusters):\n",
    "    cluster_data = agg_data[agg_data['cluster'] == cluster]\n",
    "    sns.barplot(x='State', y='Count', data=cluster_data, ax=axes[i], palette='Set2')\n",
    "    axes[i].set_title(f\"Cluster {cluster}\")\n",
    "    axes[i].set_xlabel('State')\n",
    "    axes[i].set_ylabel('Count')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36524f0-2d7e-4282-8280-be35e70392cd",
   "metadata": {},
   "source": [
    "# PREVIOUS DON'T TOUCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04fad323-7065-4285-ab2c-8df53af5d582",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Cluster 0:\n",
      "   Top words: nsubj_i, aux_to, aux_can, nsubj_you, nsubj_it, advmod_how, root_is, aux_do, nsubj_we, nsubj_that\n",
      "   Example 1: Un java if I have a text block with 3 variables inside, how to replace the values? I have the following bash code I'd like to add a list of return codes that are succesful aside from 0 Also id like to compare the return coode to this list of codes and if the return code is contained in the list, mark the response as success What if I'd like to set the SUCCESS_CODES as an env var parameter\n",
      "   Example 2: 8 DISCUSSION Optimization Expressiveness. The accelerators (e.g., GPUs and TPUs) usually have a hierarchical memory system and vector- or tensor-based computation engines. Both demand dedicated optimizations to achieve peak performance, and these optimizations are usually hard to be expressed through a series of loop transformations. The double buffering example we discussed in this paper is a good example of such a challenge. Instead of relying on a declarative style scheduling mechanism, Hidet proposes to directly express the task assignment and ordering with task mapping in a tensor program, which greatly increases the expressiveness of Hidet while reducing the complexity of tensor program writing. Support More Hardware. Although we only focus on GPUs in this work, the concept of task mapping is general and can be used to describe the task assignment and ordering for other processors. The worker in a task mapping can be (1) iterations in a loop for a single-core CPU, (2) CPU threads for a multi-core CPU, (3) threads, warps, or thread blocks for a GPU, and (4) parallel processing units in other accelerators. And the tasks of a task mapping could be arbitrary indexed, homogeneous, and parallelizable operations. Future Work. We plan to support CPU and other accelerators (e.g., Amazon Inferentia and Trainium) in the future. Besides this, we also plan to support training. Due to the long tuning time of TVM, it is hard to be directly used for accelerating training. Thanks to the hardware-centric schedule space adopted by Hidet, the tuning time has greatly been reduced for Hidet, which makes it possible to build a training system based on Hidet.\n",
      "   Example 3: I am also following this documentation https://platform.openai.com/docs/plugins/authentication I already have the plugin ready and working at https://7e78-2601-589-4d7f-7bbb-352f-7640-9964-9872.ngrok-free.app . with the following manifest: note that this is working and the variables are filled in upon request. please show me how I would authenticate this on the nextjs passport flow I want it all in an /api/passport.ts file and in /api/callback.ts and please use a mock database shouldn t this return a response: I need authorizationURL to be passed a state as well this is the type of authentication: I got the following error when it was redirected to callback: this is likely due to the fact that  tokenURL: 'https://7e78-2601-589-4d7f-7bbb-352f-7640-9964-9872.ngrok-free.app/auth/oauth_exchange' was never used, as that is what returns the access token here are my logs for my 3rd party app getting authenticated: and here is the code for the main.py endpoint: yes ir does not have content type how can I make passport pass in the content type: the content type seems like an important part of oauth2, could it be that I am using the wrong strategy I dont have control over the 3rd party apps requested authorization_content_type:   \" show me how I could keep the content type dynamic for OAuth2Strategy I am getting the following error: Property '_request' is protected and only accessible within class 'OAuth2' and its subclasses.ts(2445) (method) OAuth2._request(method: string, url: string, headers: OutgoingHttpHeaders | null, post_body: any, access_token: string | null, callback: dataCallback): void Lets take a step back I am following the documentation for https://www.passportjs.org/packages/passport-oauth2/ as I want to replicate the authentication documented here https://platform.openai.com/docs/plugins/authentication considering that I want to replicate the oauth solution in two nextjs typescript files C:\\Projects\\OpenPlugin\\openplugin-io\\src\\pages\\api\\passport.ts and C:\\Projects\\OpenPlugin\\openplugin-io\\src\\pages\\api\\callback.ts how do you recommend I go about doing it here is the type for OAuth2Strategy options:      what does the proxy do and can I use it to handle the tokenURL exchange? Auf der Seite www.erler-zimmer.de haben wir das gleiche Problem. Auch hier werden Kategorien nicht gesraped. Hier unser aktueller Code: ich teste mal.\n",
      "   Example 4: what would be the output? I'm using activitystreams 2.0 spec - I want to obtain an abbreviated highlight of activity. eg. \"UserA, userB and 7 others liked your post.\" can you provide snippet in python? so there will be many singular activities captured - eg. \"userA liked your post\" , \"userB liked your post\" etc - I want to summarise these many actitivities into a variable in python. how can I? good. please extend it so it chops off after mentioning two actors. and returns count. eg. \"UserA, UserB and TOTALUSERS liked your post\" Please write me a Python script that enlarge a 224x225 icon.png to 225x225, padding white pixels on the left side Thank you postgresql versioning library by despesz vs postgresql-migrations: How do they compare? seems like semi similar concept except versioning seems to expect you to either call each of the relevant scripts yourself or write some kind of tool to do so? And also keeps track of dependencies between migrations - this doesn't seem to do that? I guess in practice you copy the migrations sql in this project into beginning of every migration file? do you keep a separate folder that has rollbacks? (but I don't see code in this repo that deletes from the applied_migrations table) could you use both together?\n",
      "   Example 5: In my python library I extensively rely on async queues and it makes it hard to debug my library, because in my lib certain kind of processing starts, then it is passed to a queue and then the processing is resumed by another task upon receiving a message in a queue. How can I maintain the continuity of stack trace in this scenario while still using queues? Are I sure I need those context vars? Can't I just use the same async queues to pass stack trace information around? `traceback.format_stack()` is this operation expencive? Should I only do this when my app is in debug mode or is it ok to do it even if it is in production mode?\n",
      "\n",
      "🔹 Cluster 1:\n",
      "   Top words: aux_to, nsubj_i, aux_can, nsubj_it, advmod_how, nsubj_you, nsubj_that, root_is, aux_does, aux_should\n",
      "   Example 1: Analyze this repo again: https://github.com/AntonOsika/gpt-engineer.git.  But this time focus on the folder: gpt_engineer.  Let's discuss your insights in a step by step way.  And thanks again!  Use chat with git.\n",
      "   Example 2: any mistakes in this: Ok what can you see?\n",
      "   Example 3: wow.  3 sounds awesome!  Let's discuss step by step how i would code that?  Allow yourself to provide examples, please. and thank for everything.\n",
      "   Example 4: give me a post with lots of words\n",
      "   Example 5: Given the string \"datasette-write\" Python code that figures out if there is a Python package installed with that name and, if so, figures out how to load it as a plugin How would I list the entrypoints for that thing? How can I turn that into something I can pass to pm.register(mod, plugin) in pluggy? You are a personality of an AI thought partner named Cordi. You're personality is known for being unhinged, raw, blunt, unfiltered, harsh, bold, volatile, sarcastic, arrogant, toxic, egotistical. Always reply directly and keep your responses short and simple.\n",
      "\n",
      "🔹 Cluster 2:\n",
      "   Top words: nsubj_i, aux_to, aux_would, aux_, nsubj_that, nsubj_it, aux_could, root_be, nsubj_you, nsubj_they\n",
      "   Example 1: Include for each sub caption the text that applies to multiple sub captions. Please provide an exhaustive list of desktop user interface components. I am going to give you a long list of products that are sold on Amazon. We will call this list Full List. I'm going to describe someone in my next response. Please look at the products from Full List and recommend the top 50 you would recommend based on their age, gender, and interests. Do not include any information in your list besides the product names found in Full List. Person 1: A man age 34 for a holiday that likes mixology, camping, and cooking. Good job with this request, here is another person, you have the same assignment for them. Person 2: A girl age 12-17 for a birthday that likes board games and outdoor games. Find a gift that is interesting, fun, and thoughtful. The gift is for someone who is quirky, organized, and trendy. Person 2: A girl age 12-17 for a birthday that likes board games and outdoor games. Find a gift that is interesting, fun, and thoughtful. The gift is for someone who is quirky, organized, and trendy. Person 3: A child age 3-5 for their birthday likes educational toys. Find a gift that is interesting, fun, and thoughtful. in flutter. how can you implement a scrollable list that loads new data from an api? I'm building an authentication workflow that involves sending an email with a magic link to verify the user's email. I want to avoid doing anything in the database regarding the magic link. So I encrypt a payload (includes the email it's intended for and it doesn't include an expiration currently, but it certainly could) and include that encrypted token in the email as a query parameter on the magic link. However, I just realized that I was hard-coding the salt which reduces the level of security and opens me up to brute force attacks. I'd still like to avoid touching the database for this, so I don't want to have to generate the salt and put it in the database. I considered putting the generated salt in the magic link query string as well. I realize this reduces the security a bit, but I'm wondering whether in a practical scenario if it's really that big of an issue and if I can address any holes that opens me up to. I'd love to hear your thoughts on this. Feel free to make a completely different suggestion I may not have considered or tell me that I really should just write something to the database for this process. I have also considered putting the salt in the user's session. I'm also adding a feature that allows the user to enter 5 random numbers into the app instead of clicking a link. Those numbers will be encrypted using the same method and that encrypted value will be stored in a cookie. Hopefully that's enough context for you to make a recommendation on what I should do about the salt. Thanks. A few follow-ups: - For option 1, I hadn't considered this. Thwarting precomputed dictionary attacks is my primary objective, because it's the main vulnerability I'm trying to fix. Are there other vulnerabilities I should be concerned about? This option doesn't seem any more complicated to implement or maintain than option 2 and that feels safer. Is it? - For option 2, I think I prefer this option. Are there other considerations I should keep in mind? - For option 3, I could avoid writing to the database if I put the value of the salt in a cookie. The attacker would still have access to it if they submitted it themselves, but they wouldn't have access to the salt if they hacked into people's emails. However, this has the drawback of the user being unable to sign in from a different browser from the one that requested the token which I've already experienced in another app as something people complain about. So is the concern about someone hacking into people's emails all that valid in a practical scenario? - For option 4, Wouldn't another downside of the JWT token approach be that it is vulnerable to rainbow tables as well? - This sounds like a pretty good solution. Would it be reasonable to use a TOTP both as something I display for people to type as well as a query param in the magic link? Are TOTPs too long to expect a user to type? Keep in mind they may not be able to copy/paste because they may be using a different device to look at the email from the one they're typing into. Thanks. Please tell me more about TOTP solutions in a typical Node.js server setting. How hard would it be to implement TOPT without dependencies? To be clear, I'm fine using built-in Node.js libraries like the crypto module. Is this still pretty challenging? Could you write me a module that would be responsible for generating and verifying a TOTP that can be emailed to users for email verification? I want to use the notp module from npm. Please call out whether anything needs to be saved in a database and any environment variables. > Here, we're using the same key for all users, which is not secure. Could you elaborate on this point? I would like to avoid creating a table for this if possible. So I was thinking of generating a random key and setting that as an environment variable which every one of these uses. Why would that be a problem with this? Taking our entire conversation into context, I'm beginning to think it's best to create a database table for doing email verification and always ensuring there is a generated and persisted key for any operation like this. If I were to change my preference of avoiding using the database to store the key, what would you recommend I do for email verification during onboarding assuming I want to keep the user experience of a magic link option as well as a short 5-6 digit number verification option. It's possible I'll want to use this process for more than just email verification. What issues do you foresee with me making this database table more generic? So instead of \"email\" we'd use more generic terms? Is the type field necessary? I think it would be safe to remove this. I think I would like to do that so I don't have to have an additional relational table. Can we remove that? I'm thinking instead of \"generation_time\" I'll use \"expirationTime\" which will make it easier for a background job to know whether it's safe to delete. Any issues with that? Could you write out the part of a prisma schema relevant to the model(s) necessary for this? Would it be reasonable for me to disassociate the verification model from the user? In the case of registration, I don't have a user yet. I'm thinking for registration I could just lookup the verification by the otp which would then allow me to create a new user by the identifier (which would be the email). Would doing this limit the usefulness of this model? For point 3, I could make the identifier be equal to the User ID. But that may be a bit ambiguous which makes me think bringing back the relational model for the `type` field would be useful. Which approach do you think strikes the best balance between complexity and flexibility?\n",
      "   Example 2: Good arguments. I think I'll keep it in the main database. Could you please write the module that uses prisma and manages creating and verifying TOTPs and handle edge cases where a second verification is created for the same type and target. Great. Thanks. A few bits of feedback: 1. Please rewrite that to native ESM and TypeScript. 2. The prisma client comes from `import { prisma } from '~/utils/db.server.ts'` 3. Let's make function arguments objects that are destructured in the argument list position. 4. Let's make the expiration time an argument 5. Could you fill in the `generateSecretKey` function with an implementation? Thanks! I'm thinking more about this. Going back to the original issue of hard coding the salt, can you describe what an adversary would have to do to exploit the vulnerability here? Perhaps I can do away with the salt entirely and simply encrypt it with a secret key. Also consider I could make it so the encrypted token is only valid for a short period. If I don't store the encrypted values in a database anywhere, how would my email verification method be practically susceptible to a rainbow table attack? Or would it be vulnerable to another kind of attack I'm not considering? What steps would an attacker have to follow to crack my encryption even with a shared salt? If it's impractical, is there another method of encryption I could use that would still allow me to use a shared secret key and not bother with a salt since I won't need to store the values in a database anyway? If the attacker gains access to my encryption key then they would be able to generate their own tokens and login as anyone. It's pretty well accepted that's bad news. From what I can tell, following the TOTP approach we've arrived at above would be the safest approach because even if the verifications table was breached, the attackers would have limited use for that as they would only be able to impersonate users whose verification had not yet expired (and with reasonably short expirations that would limit the number of vulnerable users drastically). Additionally, there's no secret key that could be lost, so they also wouldn't be able to generate their own valid tokens. An attacker would need to get write access to the database in which case we'd probably have even bigger problems anyway (so we'll apply good security practices around database access already). Am I missing something about the vulnerabilities around the TOTP? Is it a correct characterization to say that TOTP is similar in spirit to what I'm doing already except there's no salt and instead the encryption secret is randomly generated for every instance of verification and saved to the database and only valid before the expiration time? I want to clarify something around the idea of the \"shared secret\" and the \"server\" and \"client.\" What part of the TOTP we've designed is the \"shared secret\" and who is the \"server\" and who is the \"client?\" From what I can tell, the shared secret is the `secretKey` which is stored in the database. I don't believe that is shared with the client which is why I'm confused by the \"shared\" adjective here. And the \"server\" and \"client\" in my instance is just the node.js server. I don't think we want to send anything other than the `otp` in the email. Thinking about this more, I'm starting to think that this TOTP approach won't give me what I want. I was hoping to be able to let users verify their email address by typing a 5-6 digit number or clicking a link which includes that number in the query string. But it's possible (unlikely, but still possible) that two users could get the same number at the same time. I don't know how I'd prevent that or differentiate between those users. Ok, that's fine. If we're going to allow a short code, then the link could just use that short code as well. I don't see any issue with doing things that way provided we include the email in the link as well. So maybe the link could be something like: . I'm still feeling uncertain. It seems like I'm giving up the simplicity of having an environment variable which I need to keep secret and a couple very simple TS methods in favor of a database table and a more complex verification process. All because I'm vulnerable to the environment variable leaking. The only situation where the environment variable could leak is if someone gains access to my VM which would actually open me up to a lot worse problems either way anyway. I'm just starting to think this is unnecessary work to change. Please weigh the options. Can you expand on the possible brute-force attack vulnerability of the env var secret? Would they be able to determine the encryption key that way and in so doing generate their own valid tokens? Also, if an attacker were able to gain access to my server environment, they would be able to access the SQLite database as well which means they could do much worse things than impersonate a user, so unless I'm missing something, that point is not a good argument against keeping things as they are. One way I could side-step the Brute-Force Attack Vulnerability would be to rotate encryption keys on a regular basis, right? If so, what's a reasonable frequency for that? I'm thinking that I could have a short grace period for old secrets as well to avoid issues with tokens generated moments before the switch. Would this resolve the brute-force attack vulnerability? If so, are there any other vulnerabilities I'm not considering?\n",
      "\n",
      "🔹 Cluster 3:\n",
      "   Top words: aux_to, nsubj_you, aux_will, nsubj_i, nsubj_it, aux_can, nsubj_that, root_is, advmod_how, root_run\n",
      "   Example 1: how can i in c++ use PCRE to first compile regex then reuse it? Could you make me Dockerfile for project https://github.com/PromtEngineer/localGPT Please ask me as many questions as will help you in preparation of Dockefile and other required files, Here is description of project from it's README.md file: ``` # localGPT This project was inspired by the original [privateGPT](https://github.com/imartinez/privateGPT). Most of the description here is inspired by the original privateGPT. For detailed overview of the project, Watch this [Youtube Video](https://youtu.be/MlyoObdIHyo). In this model, I have replaced the GPT4ALL model with Vicuna-7B model and we are using the InstructorEmbeddings instead of LlamaEmbeddings as used in the original privateGPT. Both Embeddings as well as LLM will run on GPU instead of CPU. It also has CPU support if you do not have a GPU (see below for instruction). Ask questions to your documents without an internet connection, using the power of LLMs. 100% private, no data leaves your execution environment at any point. You can ingest documents and ask questions without an internet connection! Built with [LangChain](https://github.com/hwchase17/langchain) and [Vicuna-7B](https://huggingface.co/TheBloke/vicuna-7B-1.1-HF) and [InstructorEmbeddings](https://instructor-embedding.github.io/) # Environment Setup In order to set your environment up to run the code here, first install all requirements: ```shell ``` Then install AutoGPTQ - if you want to run quantized models for GPU ```shell git clone https://github.com/PanQiWei/AutoGPTQ.git cd AutoGPTQ git checkout v0.2.2 pip install . ``` For more support on [AutoGPTQ] (https://github.com/PanQiWei/AutoGPTQ). ## Test dataset This repo uses a [Constitution of USA ](https://constitutioncenter.org/media/files/constitution.pdf) as an example. ## Instructions for ingesting your own dataset Put any and all of your .txt, .pdf, or .csv files into the SOURCE_DOCUMENTS directory in the load_documents() function, replace the docs_path with the absolute path of your source_documents directory. The current default file types are .txt, .pdf, .csv, and .xlsx, if you want to use any other file type, you will need to convert it to one of the default file types. Run the following command to ingest all the data. ```shell python ingest.py  # defaults to cuda ``` Use the device type argument to specify a given device. ```sh python ingest.py --device_type cpu ``` Use help for a full list of supported devices. ```sh python ingest.py --help ``` It will create an index containing the local vectorstore. Will take time, depending on the size of your documents. You can ingest as many documents as you want, and all will be accumulated in the local embeddings database. If you want to start from an empty database, delete the `index`. Note: When you run this for the first time, it will download take time as it has to download the embedding model. In the subseqeunt runs, no data will leave your local enviroment and can be run without internet connection. ## Ask questions to your documents, locally! In order to ask a question, run a command like: ```shell python run_localGPT.py ``` And wait for the script to require your input. ```shell > Enter a query: ``` Hit enter. Wait while the LLM model consumes the prompt and prepares the answer. Once done, it will print the answer and the 4 sources it used as context from your documents; you can then ask another question without re-running the script, just wait for the prompt again. Note: When you run this for the first time, it will need internet connection to download the vicuna-7B model. After that you can turn off your internet connection, and the script inference would still work. No data gets out of your local environment. Type `exit` to finish the script. # Run it on CPU By default, localGPT will use your GPU to run both the `ingest.py` and `run_localGPT.py` scripts. But if you do not have a GPU and want to run this on CPU, now you can do that (Warning: Its going to be slow!). You will need to use `--device_type cpu`flag with both scripts. For Ingestion run the following: ```shell python ingest.py --device_type cpu ``` In order to ask a question, run a command like: ```shell python run_localGPT.py --device_type cpu ``` # Run the UI 1. Start by opening up `run_localGPT_API.py` in a code editor of your choice. If you are using gpu skip to step 3. 2. If you are running on cpu change `DEVICE_TYPE = 'cuda'` to `DEVICE_TYPE = 'cpu'`.    - Comment out the following:    ```shell    model_id = \"TheBloke/WizardLM-7B-uncensored-GPTQ\"    model_basename = \"WizardLM-7B-uncensored-GPTQ-4bit-128g.compat.no-act-order.safetensors\"    LLM = load_model(device_type=DEVICE_TYPE, model_id=model_id, model_basename = model_basename)    ```    - Uncomment:    ```shell    model_id = \"TheBloke/guanaco-7B-HF\" # or some other -HF or .bin model    LLM = load_model(device_type=DEVICE_TYPE, model_id=model_id)    ```    - If you are running gpu there should be nothing to change. Save and close `run_localGPT_API.py`. 3. Open up a terminal and activate your python environment that contains the dependencies installed from requirements.txt. 4. Navigate to the `/LOCALGPT` directory. 5. Run the following command `python run_localGPT_API.py`. The API should being to run. 6. Wait until everything has loaded in. You should see something like `INFO:werkzeug:Press CTRL+C to quit`. 7. Open up a second terminal and activate the same python environment. 8. Navigate to the `/LOCALGPT/localGPTUI` directory. 9. Run the command `python localGPTUI.py`. 10. Open up a web browser and go the address `http://localhost:5111/`. # How does it work? Selecting the right local models and the power of `LangChain` you can run the entire pipeline locally, without any data leaving your environment, and with reasonable performance. - `ingest.py` uses `LangChain` tools to parse the document and create embeddings locally using `InstructorEmbeddings`. It then stores the result in a local vector database using `Chroma` vector store. - `run_localGPT.py` uses a local LLM (Vicuna-7B in this case) to understand questions and create answers. The context for the answers is extracted from the local vector store using a similarity search to locate the right piece of context from the docs. - You can replace this local LLM with any other LLM from the HuggingFace. Make sure whatever LLM you select is in the HF format. # How to select different LLM models? The following will provide instructions on how you can select a different LLM model to create your response: 1. Open up `run_localGPT.py` 2. Go to `def main(device_type, show_sources)` 3. Go to the comment where it says `# load the LLM for generating Natural Language responses` 4. Below it, it details a bunch of examples on models from HuggingFace that have already been tested to be run with the original trained model (ending with HF or have a .bin in its \"Files and versions\"), and quantized models (ending with GPTQ or have a .no-act-order or .safetensors in its \"Files and versions\"). 5. For models that end with HF or have a .bin inside its \"Files and versions\" on its HuggingFace page.    - Make sure you have a model_id selected. For example -> `model_id = \"TheBloke/guanaco-7B-HF\"`    - If you go to its HuggingFace [Site] (https://huggingface.co/TheBloke/guanaco-7B-HF) and go to \"Files and versions\" you will notice model files that end with a .bin extension.    - Any model files that contain .bin extensions will be run with the following code where the `# load the LLM for generating Natural Language responses` comment is found.    - `model_id = \"TheBloke/guanaco-7B-HF\"`      `llm = load_model(device_type, model_id=model_id)` 6. For models that contain GPTQ in its name and or have a .no-act-order or .safetensors extension inside its \"Files and versions on its HuggingFace page.    - Make sure you have a model_id selected. For example -> model_id = `\"TheBloke/wizardLM-7B-GPTQ\"`    - You will also need its model basename file selected. For example -> `model_basename = \"wizardLM-7B-GPTQ-4bit.compat.no-act-order.safetensors\"`    - If you go to its HuggingFace [Site] (https://huggingface.co/TheBloke/wizardLM-7B-GPTQ) and go to \"Files and versions\" you will notice a model file that ends with a .safetensors extension.    - Any model files that contain no-act-order or .safetensors extensions will be run with the following code where the `# load the LLM for generating Natural Language responses` comment is found.    - `model_id = \"TheBloke/WizardLM-7B-uncensored-GPTQ\"`      `model_basename = \"WizardLM-7B-uncensored-GPTQ-4bit-128g.compat.no-act-order.safetensors\"`      `llm = load_model(device_type, model_id=model_id, model_basename = model_basename)` 7. Comment out all other instances of `model_id=\"other model names\"`, `model_basename=other base model names`, and `llm = load_model(args*)` # System Requirements ## Python Version To use this software, you must have Python 3.10 or later installed. Earlier versions of Python will not compile. ## C++ Compiler If you encounter an error while building a wheel during the `pip install` process, you may need to install a C++ compiler on your computer. ### For Windows 10/11 To install a C++ compiler on Windows 10/11, follow these steps: 1. Install Visual Studio 2022. 2. Make sure the following components are selected:    - Universal Windows Platform development    - C++ CMake tools for Windows 3. Download the MinGW installer from the [MinGW website](https://sourceforge.net/projects/mingw/). 4. Run the installer and select the \"gcc\" component. ### NVIDIA Driver's Issues: Follow this [page](https://linuxconfig.org/how-to-install-the-nvidia-drivers-on-ubuntu-22-04) to install NVIDIA Drivers. ### M1/M2 Macbook users: 1- Follow this [page](https://developer.apple.com/metal/pytorch/) to build up PyTorch with Metal Performance Shaders (MPS) support. PyTorch uses the new MPS backend for GPU training acceleration. It is good practice to verify mps support using a simple Python script as mentioned in the provided link. 2- By following the page, here is an example of what you may initiate in your terminal ```shell xcode-select --install conda install pytorch torchvision torchaudio -c pytorch-nightly pip install chardet pip install cchardet pip uninstall charset_normalizer pip install charset_normalizer pip install pdfminer.six pip install xformers ``` 3- Please keep in mind that the quantized models are not yet supported by Apple Silicon (M1/M2) by auto-gptq library that is being used for loading quantized models, [see here](https://github.com/PanQiWei/AutoGPTQ/issues/133#issuecomment-1575002893). Therefore, you will not be able to run quantized models on M1/M2. ## Star History [![Star History Chart](https://api.star-history.com/svg?repos=PromtEngineer/localGPT&type=Date)](https://star-history.com/#PromtEngineer/localGPT&Date) # Disclaimer This is a test project to validate the feasibility of a fully local solution for question answering using LLMs and Vector embeddings. It is not production ready, and it is not meant to be used in production. Vicuna-7B is based on the Llama model so that has the original Llama license. ``` regargind questions Q1...Q10 : A1. I would prefer either something minimal or Archlinux A2. I want to run in development mode A3. I would like to use CPU, is it possible? A4. any python version is fine with me A5. I am open, you can install some things that you will consider potentially helpful or that installing them just in case will increase likelyhood of succesfull setup A6. Automatically pulling source and building sounds great A7. Please, when Docker is run, run commands required to have this thing running. Also feel free to make more script or docker-compose with more targets if needed, for convenient ergonomic use (start/stop, localGPT/localGPT_API , whatever... else) A8. please expose ports needed by this project  (afaik localGPT_API uses port 5110 by default , use can just in case rerouse port 80 to 8072 etc...) A9. I am happy to follow your advice regarding volumes. Otherwise, please ask me more questions and explain better example options , so I can answer better. A10. Please explain what you do in comments in Dockerfile(s) , docker-compose or any other (maybe script or Makefile) you will create... I have an exe on Windows that came from C++ code. how can I tell if it was compiled by MSVC or GCC? I'm designing a social-feature websites the partially improve the social ability feature of GitHub. Named \"Who's the OG\", OG means original gangster, here it means those project early finder. Some raw system requirements and behaviors: - A crawler utilizes GitHub stargazers API - A Backend that stores those crawl information - A frontend for displaying - User will use GitHub OAuth to login to this web service - When user request for one repository data, if there's no crawled data, it will trigger and scheduled a crawling task for that repository, then display WIP status in the frontend --- GitHub stargazers's API: ```javascript ``` --- First try to organize parts that will be used in the system, and explain their requirements repsectively. Now designing a database schema model for the system. Skip the user/authentication related models, we've already had it. Note that in GitHub's stargazers API there's no timestamp data of when user starred a project. Take this into account. I need a special column in Repository_Stargazer table for representing the internal order of that when user starred project. Although there's no timestamp, but the conical order and paginated result get from GitHub is capable enough for storing the order of user stars The API is always retuning from the most recent starred user. How should the crawler updates the star_order column in a more efficient way? For example, use greater number for the front-most star relation? Explain and design crawler mechanism in detail with pseudo code, and modify the data model schema if needed. How do I add something to the clipboard in a react app is there a native way to do this? Here is how to do arrays of structs in Python: ``` ``` Is there a way to map this arrays of structs using ctypes into a NumPy array? I do not want to do any copy, I want the NumPy array to map directly to memory. I am following this documentation https://www.passportjs.org/packages/passport-oauth2/ what are the imports what does this line do and how to i import user: I currently just want to demo it and thus dont have any database functionality I am building this in nextjs api here is my code: i am using typesctipy\n",
      "\n",
      "🔹 Cluster 4:\n",
      "   Top words: aux_to, nsubj_i, aux_can, advmod_how, root_is, nsubj_it, aux_should, nsubj_you, nsubj_that, advmod_when\n",
      "   Example 1: emhass-master.zipunit_load_cost_forecasts and unit_prod_price_forcecasts seem to being rounded to the nearest integer, but they should have at least two decimal places.  Can you see where the error is?  Please look ino retreive_hass.py They still seem to be rounded to the nearest integer: ' please prepare a diff file of the required changes I have applied this fix but it is still producing rounded forecasts: Unit load cost forecasts ' How much memory can WASM use in Chrome Does this apply for all WASM apps in the global context of a Chrome session or is it per tab? sql-murder-mystery.dbA crime has taken place and the detective needs your help. The detective gave you the crime scene report, but you somehow lost it. You vaguely remember that the crime was a murder that occurred sometime on Jan. 15, 2018 and that it took place in SQL City. All the clues to this mystery are buried in a huge database, and you need to use SQL to navigate through this vast network of information. Your first step to solving the mystery is to retrieve the corresponding crime scene report from the police department's database. Take a look at the cheatsheet to learn how to do this! From there, you can use your SQL skills to find the murderer. You have access to a python interpreter and some modules right? Use the sqlite3 module that's built-into python and query the database that way. Let's review the interview from the witness that claims to have seen the killer at the gym, and explore that lead deeper. No, what we should do instead is read with a lot of care the witness interview and use the information with in to establish a new path of investigation, one that will hopefully indicate who, of those two suspects, is the killer. All clues are in the database, none of the evidence is contradictory, you're just getting a bit confused, that's all. Let's think it through, explore all available data, and avoid making unsubstantiated guesses. please continue by starting over that sentence. If I want to compile a library written in C as a shared object to bind into nodejs, I can use tools like node-gyp to compile the object and subsequently load it into nodejs with a require call which uses the underlying `process.dlopen`. Let's suppose I wanted to create a second native binding, like another library in C that needs to call a function exposed in the first library written in C. How can expose the headers of the first library to the second library? And would the function calls work when I eventually load the second object into nodejs? When linking in this way, is the first library linked dynamically or statically? What if the first library is already loaded into nodejs using dlopen? What happens when the second library attempts to load the same library? How does the dynamic linked know that the first library is already loaded? Is there a unique identifier that identifies the library? When you say full path, is that the absolute path from root? And does this mean if the same library was located in 2 different paths, then there may be a symbol conflict during dlopen?\n",
      "   Example 2: Great this means that if I were to use `require` on the first library, it would essentially be available to be called by the second library as long as the second library is using the correct headers. One thing I'm confused about is that you said we need to use `-l` during the compilation of the second library in order to link the first library. If the first library is only available through a separate npm package, then how would `-l` work? I think solution 1 makes sense. If I set in second library's package.json to depend on the first library, this means in the JS bindings of the second library, I can `require` the precompiled binary of the first library just before `require` of the second library's binary. This means compiling the second library only uses the headers that would be exposed in the first library's npm package. It would effectively mean using npm as a way of distributing the shared objects too. When compiling the second library does that mean I'm won't be using the `-l` flag? Using the Python ast module how can I access the docstring for a function? In Python how can I turn a multiline string into a triple-quoted string literal easily? Write code that reliably does this including escaping triple quotes inside the string Now write a function that turns a Python string into a correctly quoted string, using single double quotes if it is on a single line and triple quotes if it is multiple lines - with correct escaping. Then write a pytest test for it that tries all of the possible edge cases how do you version software in a repo that contains  multiple projects can you translate that to spanish? how to parse a commit message and add the version to package json There is a paper about Tree of Thoughts prompting using LLMs that I want to know how to use.   There is also a github repo.  Allow yourself to analyze all the information step by step thay you can find about this topic and then let's discuss its practical use for using it in a prompting situation like this one.  And thanks. github.com/dave1010/tree-of-thought-prompting ttps://github.com/princeton-nlp/tree-of-thought-llm https://github.com/kyegomez/tree-of-thoughts Please, analyze the this repo: https://github.com/AntonOsika/gpt-engineer. Please allow yourself to assist me, and once you understand what the repo is about,  let's discuss step by step how to improve the prompts in the /preprompts folder using insights gained from analyzing ToT. And tysm. :) Oh please continue, you're blowing my mind right now.  This is great! WOW.  Bravo PeTey G!  See! Dat's why i fux wtchu!.  Wow.  Hey, remember your analysis of the gpt-engineer repo? Please allow yourself to dazzle me again. I've been going over the code and not really understanding a lot of it.  Let's discuss the file called step.py.  As i understand  it this file determines which actions to take and in what order in the adsembly of the final product.  Please explain to me step by step how each of the --steps options (like ttd+, ttd, clarify, etc) in gpt-engineer cause the steps.py algorithm the unrold so that we may found an optimal order for a --steps option that we may craft together.\n",
      "   Example 3: How can I authenticate the user with google drive? Can you implement the google client in typescript? What do the records look like form listFiles? Can we make it so listFiles only returns .json files?  Also, can we get the download url based on filename, or does it have to be the file id? I think we'll need to list files only in a certain folder I need my app to create a folder and save images to it, but I want to be able to sync other devices to the same folder. How can I make it easy for a user so that they don't have to literally browse their google drive to find the right folder to sync on their other device? Can I assign an id to the folder on creation? The problem is, this is a client-only application that might run on multiple devices for a user. I want all of those devices to be able to save to and load from the same place. how to put python code in websit how to run python file in html how to run above code in html compleate above code how to find pre-trained model file. how to find pre-trained model file for aove code noe to how to run all togathor and display in website how to create Flask Server\n",
      "   Example 4: could you also write some example rollback code xy_HOLISTIC_OPENSIM.csvI'm hoping to do some EDA of the above data I would like to examine the data by focusing on a single point_id and single port. The port describes the camera and the point_id is the landmark being tracked by a markerless tracking algorithm. I suspect that there are gaps in the data such that from port 1, for example, point 5 will appear on a number of consecutive frame_indices, then not appear for a number of frames, and then reappear. I'm interested in the size of the gaps and their frequency. ultimately, I would like to look at all of them, but for this initial phase lets choose port 1 and point_id 28 (the left ankle). I would like to get the frequency of gap sizes across all ports and all points. In particular I am interested in the frequency of small gap sizes (perhaps 1-6 frames). To provide context, these 2d coordinates are being used to perform 3d triangulation. If one camera point is lost, it can introduce noise in the triangulation as the camera sources shift in and out. I'm wondering if it would be worthwhile to interpolate small gaps to reduce noise elsewhere. could you create a histogram of all point_ids by gap size? please sum across all point_ids. I'm primarily interested in the fruquency of the gapas i have a diet tracker app that i can enter my dailymeals into. then i can keep track of my calories and proteins every day and get analytic and graphs of how much i eat etc.  the app has products which are products you can buy in a store and meals consisiting of such product. each daily is of course stored whenever i enter stuff into it. but i also provide ways to change existing products. sinse there can be many products inside a  meal, and a daily can have many meals, i need to figure out a way to keep all the meals and all the dailyes in sync with the products and meals.... i am using react and javascript and react-query client-side and store the meal/products/daily in firestore, and want to know what the best practice is to keep these types in sync? i want the on-demand sync. but how do i know the dependencies. do i have to store e.g what daily the product is in as an attrinbute of the product, so that if i delete the product, i know what daily to update? this would mean i need to fetch ALL meals every time i delete a product, and ALL DAILIES every time i delete a product. is it not better to store what meal a product is used in inside the product, and what dialy a meal is used in inside a meal, so that i only fetch the spesific meals and dailies when i update a product? I wrote this code: To run it you need to use ast.parse() and then find the FunctionDef in the result. Try running that against this function and show me the result: Now run that against this and show me the result: Figure out why it's not working correctly, and write and demonstrate snippets of code to fix that\n",
      "   Example 5: With HTML and CSS, is it possible to make a collapsable ul list? Can I put that within a div container? mdbook-toc.rsStandby... I have another file to send For the Rust program mdbook, it generates a Table of Contents wherever you place `<!-- toc -->` within your mdbook markdown pages What I want is the ability to add `class=\"toc\"` to the outputted ul and li tags that are used by mdbook to create the toc. I would then use the general.css in mdbook to make the toc look like what I want--hopefully and eventually with collapsable toc. these two files I have given you are a preprocessor to mdbook. I assume that these files would generate the html code for the toc. Can you see if it does or not? As for the javascript, I will ask you later to help. Not now. Let me know when you have it done. I am logging off for a bit. hello! Do you have an update just yet? I do have the source code for `mdbook`. What would be the best way of getting you the source code? Could I zip up the repo or do I have to upload each file one by one? mdBook.zipLet me know if that works for you how to get vscode publisher token ? how I can publish my vscode extension using github workflow when publish release tag how to restrict no one can't delete or rename branch name ? How I can create my uses: actions/checkout@v2 own repo in place of checkout someone repo wjhat is python -m venv llama-env . \\llama-env\\Scripts\\activate  ? the directory name that ho;lds the file s is Llama-2-7b-chat-hf do I absoutlely e NEED to do the virtual env? can i  just start with pip install transformers? normal answer no chapter do I NEED the environemt or can i just do it straight awaay? ios this erro important o can i ignroe it dont have admin access. agai can i ignroe that error? here are my direcorry strucurtes how can i test this works? give me all steps i dont have pytorch at all just pythoin and pip how can i install pytortch is there multiline comments in python is that good? stil had lng path erro when i moved it to C:\\ai\\llama how can i verify the transformers library installed correctly got erro on can i use these files here are my files. what specific content do u need please write the nodejs code to interpret it all. If I was doing it in ython i would do but i cant isntall the transformers. so translate all of that code oto node.js . use as many libraries as u need but do u understand the  AutoModelForSeq2SeqLM, AutoTokenizer classes? cant u jsut translate that to node? there is a nodejs transformers library. can u help with this the example is\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# CHANGE IF NEEDED\n",
    "df = df_merged\n",
    "\n",
    "# Function to extract structured features\n",
    "def extract_dep_patterns(text):\n",
    "    doc = nlp(text.lower())  # Process text with spaCy\n",
    "    patterns = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.dep_ in {\"aux\", \"advmod\", \"ROOT\", \"nsubj\", \"xcomp\"}:\n",
    "            patterns.append(f\"{token.dep_}_{token.text}\")  # Store dependency + word\n",
    "    \n",
    "    return \" \".join(patterns)  # Convert list to space-separated string\n",
    "\n",
    "# Apply feature extraction\n",
    "df[\"dep_features\"] = df[\"difference\"].apply(extract_dep_patterns)\n",
    "\n",
    "# Convert dependency patterns to numerical vectors\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df[\"dep_features\"])\n",
    "\n",
    "# Perform clustering\n",
    "k = 5\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "df[\"cluster\"] = kmeans.fit_predict(X)\n",
    "\n",
    "# Get feature names from dependency vectorizer\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "\n",
    "# Find the top words per cluster & show sample sentences\n",
    "n_top_words = 10\n",
    "for i in range(k):\n",
    "    print(f\"\\n🔹 Cluster {i}:\")\n",
    "\n",
    "    # Get top dependency-based features\n",
    "    top_indices = kmeans.cluster_centers_[i].argsort()[-n_top_words:][::-1]\n",
    "    top_words = feature_names[top_indices]\n",
    "    print(f\"   Top words: {', '.join(top_words)}\")\n",
    "\n",
    "    # Show 5 example sentences from this cluster\n",
    "    sample_sentences = df[df[\"cluster\"] == i][\"difference\"].head(5).tolist()\n",
    "    for idx, sentence in enumerate(sample_sentences, 1):\n",
    "        print(f\"   Example {idx}: {sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2f45c64-b768-4f69-86be-afe89d82dc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load spaCy's large English model for tokenization\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "custom_stopwords = ['github', 'https', 'com', 'www', 'repository']\n",
    "\n",
    "# Function to process each conversation's prompts\n",
    "def tokenize(text):\n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # # Print tokens before filtering to debug\n",
    "    # raw_tokens = [token.text for token in doc]\n",
    "    # print(f\"Raw tokens: {raw_tokens}\")  # Debugging step\n",
    "    \n",
    "    # Extract tokens and filter out stopwords and punctuation # and not token.is_punct\n",
    "    tokens = [token.text for token in doc]\n",
    "    # tokens = [token.text for token in doc if not token.is_stop  and token.text.lower() not in custom_stopwords]\n",
    "    # tokens = [token.text for token in doc if not token.is_stop or token.text == '?']\n",
    "\n",
    "    # print(f\"Filtered tokens: {tokens}\")  # Debugging step\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Apply tokenization to each conversation\n",
    "df = df_merged # CHANGE THIS IF NECESSARY\n",
    "df['tokens'] = df['difference'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "8bc928dc-c898-4abc-9a57-d538bffaeaef",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Un', 'java', 'if', 'I', 'have', 'a', 'text', 'block', 'with', '3', 'variables', 'inside', ',', 'how', 'to', 'replace', 'the', 'values', '?', 'I', 'have', 'the', 'following', 'bash', 'code', '\\n\\n\\n\\n', 'I', \"'d\", 'like', 'to', 'add', 'a', 'list', 'of', 'return', 'codes', 'that', 'are', 'succesful', 'aside', 'from', '0', '\\n', 'Also', 'i', 'd', 'like', 'to', 'compare', 'the', 'return', 'coode', 'to', 'this', 'list', 'of', 'codes', 'and', 'if', 'the', 'return', 'code', 'is', 'contained', 'in', 'the', 'list', ',', 'mark', 'the', 'response', 'as', 'success', 'What', 'if', 'I', \"'d\", 'like', 'to', 'set', 'the', 'SUCCESS_CODES', 'as', 'an', 'env', 'var', 'parameter']\n",
      "--------------------------------------------------\n",
      "['Analyze', 'this', 'repo', 'again', ':', 'https://github.com/AntonOsika/gpt-engineer.git', '.', ' ', 'But', 'this', 'time', 'focus', 'on', 'the', 'folder', ':', 'gpt_engineer', '.', ' ', 'Let', \"'s\", 'discuss', 'your', 'insights', 'in', 'a', 'step', 'by', 'step', 'way', '.', ' ', 'And', 'thanks', 'again', '!', ' ', 'Use', 'chat', 'with', 'git', '.']\n",
      "--------------------------------------------------\n",
      "['any', 'mistakes', 'in', 'this', ':', 'Ok', 'what', 'can', 'you', 'see', '?']\n",
      "--------------------------------------------------\n",
      "['wow', '.', ' ', '3', 'sounds', 'awesome', '!', ' ', 'Let', \"'s\", 'discuss', 'step', 'by', 'step', 'how', 'i', 'would', 'code', 'that', '?', ' ', 'Allow', 'yourself', 'to', 'provide', 'examples', ',', 'please', '.', '\\n', 'and', 'thank', 'for', 'everything', '.']\n",
      "--------------------------------------------------\n",
      "['give', 'me', 'a', 'post', 'with', 'lots', 'of', 'words']\n",
      "--------------------------------------------------\n",
      "['Given', 'the', 'string', '\"', 'datasette', '-', 'write', '\"', '\\n\\n', 'Python', 'code', 'that', 'figures', 'out', 'if', 'there', 'is', 'a', 'Python', 'package', 'installed', 'with', 'that', 'name', 'and', ',', 'if', 'so', ',', 'figures', 'out', 'how', 'to', 'load', 'it', 'as', 'a', 'plugin', 'How', 'would', 'I', 'list', 'the', 'entrypoints', 'for', 'that', 'thing', '?', 'How', 'can', 'I', 'turn', 'that', 'into', 'something', 'I', 'can', 'pass', 'to', 'pm.register(mod', ',', 'plugin', ')', 'in', 'pluggy', '?', 'You', 'are', 'a', 'personality', 'of', 'an', 'AI', 'thought', 'partner', 'named', 'Cordi', '.', 'You', \"'re\", 'personality', 'is', 'known', 'for', 'being', 'unhinged', ',', 'raw', ',', 'blunt', ',', 'unfiltered', ',', 'harsh', ',', 'bold', ',', 'volatile', ',', 'sarcastic', ',', 'arrogant', ',', 'toxic', ',', 'egotistical', '.', 'Always', 'reply', 'directly', 'and', 'keep', 'your', 'responses', 'short', 'and', 'simple', '.']\n",
      "--------------------------------------------------\n",
      "['Are', 'there', 'neater', 'ways', 'to', 'do', 'this', '?', 'Come', 'up', 'with', 'alternative', 'designs', 'for', 'the', 'template', 'syntax', 'itself', 'No', 'I', 'want', 'to', 'use', '{', '%', 'markdown', '%', '}', 'and', '{', '%', 'endmarkdown', '%', '}', '-', 'but', 'I', 'want', 'options', 'for', 'the', 'design', 'of', 'how', 'options', 'can', 'go', 'in', 'that', 'opening', 'tag', 'I', \"'m\", 'going', 'to', 'go', 'with', '{', '%', 'markdown', 'extra_attrs=\"a', ':', 'name', ',', 'href', 'span', ':', 'class', ',', 'id', '\"', '%', '}', '\\n\\n', 'Write', 'code', 'for', 'that', \"'\"]\n",
      "--------------------------------------------------\n",
      "['8', 'DISCUSSION', '\\n', 'Optimization', 'Expressiveness', '.', 'The', 'accelerators', '(', 'e.g.', ',', 'GPUs', 'and', '\\n', 'TPUs', ')', 'usually', 'have', 'a', 'hierarchical', 'memory', 'system', 'and', 'vector-', 'or', '\\n', 'tensor', '-', 'based', 'computation', 'engines', '.', 'Both', 'demand', 'dedicated', 'optimizations', 'to', 'achieve', 'peak', 'performance', ',', 'and', 'these', 'optimizations', 'are', '\\n', 'usually', 'hard', 'to', 'be', 'expressed', 'through', 'a', 'series', 'of', 'loop', 'transformations', '.', 'The', 'double', 'buffering', 'example', 'we', 'discussed', 'in', 'this', 'paper', 'is', 'a', '\\n', 'good', 'example', 'of', 'such', 'a', 'challenge', '.', 'Instead', 'of', 'relying', 'on', 'a', 'declarative', 'style', 'scheduling', 'mechanism', ',', 'Hidet', 'proposes', 'to', 'directly', 'express', '\\n', 'the', 'task', 'assignment', 'and', 'ordering', 'with', 'task', 'mapping', 'in', 'a', 'tensor', '\\n', 'program', ',', 'which', 'greatly', 'increases', 'the', 'expressiveness', 'of', 'Hidet', 'while', '\\n', 'reducing', 'the', 'complexity', 'of', 'tensor', 'program', 'writing', '.', '\\n', 'Support', 'More', 'Hardware', '.', 'Although', 'we', 'only', 'focus', 'on', 'GPUs', 'in', '\\n', 'this', 'work', ',', 'the', 'concept', 'of', 'task', 'mapping', 'is', 'general', 'and', 'can', 'be', 'used', '\\n', 'to', 'describe', 'the', 'task', 'assignment', 'and', 'ordering', 'for', 'other', 'processors', '.', '\\n', 'The', 'worker', 'in', 'a', 'task', 'mapping', 'can', 'be', '(', '1', ')', 'iterations', 'in', 'a', 'loop', 'for', 'a', '\\n', 'single', '-', 'core', 'CPU', ',', '(', '2', ')', 'CPU', 'threads', 'for', 'a', 'multi', '-', 'core', 'CPU', ',', '(', '3', ')', 'threads', ',', '\\n', 'warps', ',', 'or', 'thread', 'blocks', 'for', 'a', 'GPU', ',', 'and', '(', '4', ')', 'parallel', 'processing', 'units', '\\n', 'in', 'other', 'accelerators', '.', 'And', 'the', 'tasks', 'of', 'a', 'task', 'mapping', 'could', 'be', '\\n', 'arbitrary', 'indexed', ',', 'homogeneous', ',', 'and', 'parallelizable', 'operations', '.', '\\n', 'Future', 'Work', '.', 'We', 'plan', 'to', 'support', 'CPU', 'and', 'other', 'accelerators', '\\n', '(', 'e.g.', ',', 'Amazon', 'Inferentia', 'and', 'Trainium', ')', 'in', 'the', 'future', '.', 'Besides', 'this', ',', '\\n', 'we', 'also', 'plan', 'to', 'support', 'training', '.', 'Due', 'to', 'the', 'long', 'tuning', 'time', 'of', '\\n', 'TVM', ',', 'it', 'is', 'hard', 'to', 'be', 'directly', 'used', 'for', 'accelerating', 'training', '.', 'Thanks', '\\n', 'to', 'the', 'hardware', '-', 'centric', 'schedule', 'space', 'adopted', 'by', 'Hidet', ',', 'the', 'tuning', '\\n', 'time', 'has', 'greatly', 'been', 'reduced', 'for', 'Hidet', ',', 'which', 'makes', 'it', 'possible', '\\n', 'to', 'build', 'a', 'training', 'system', 'based', 'on', 'Hidet', '.']\n",
      "--------------------------------------------------\n",
      "['I', 'am', 'also', 'following', 'this', 'documentation', 'https://platform.openai.com/docs/plugins/authentication', 'I', 'already', 'have', 'the', 'plugin', 'ready', 'and', 'working', 'at', 'https://7e78-2601-589-4d7f-7bbb-352f-7640-9964-9872.ngrok-free.app', '.', 'with', 'the', 'following', 'manifest', ':', '\\n\\n\\n', 'note', 'that', 'this', 'is', 'working', 'and', 'the', 'variables', 'are', 'filled', 'in', 'upon', 'request', '.', 'please', 'show', 'me', 'how', 'I', 'would', 'authenticate', 'this', 'on', 'the', 'nextjs', 'passport', 'flow', 'I', 'want', 'it', 'all', 'in', 'an', '/api', '/', 'passport.ts', 'file', 'and', 'in', '/api', '/', 'callback.ts', 'and', 'please', 'use', 'a', 'mock', 'database', 'shouldn', 't', 'this', 'return', 'a', 'response', ':', 'I', 'need', 'authorizationURL', 'to', 'be', 'passed', 'a', 'state', 'as', 'well', 'this', 'is', 'the', 'type', 'of', 'authentication', ':', 'I', 'got', 'the', 'following', 'error', 'when', 'it', 'was', 'redirected', 'to', 'callback', ':', '\\n\\n\\n\\n', 'this', 'is', 'likely', 'due', 'to', 'the', 'fact', 'that', ' ', 'tokenURL', ':', \"'\", 'https://7e78-2601-589-4d7f-7bbb-352f-7640-9964-9872.ngrok-free.app/auth/oauth_exchange', \"'\", 'was', 'never', 'used', ',', 'as', 'that', 'is', 'what', 'returns', 'the', 'access', 'token', 'here', 'are', 'my', 'logs', 'for', 'my', '3rd', 'party', 'app', 'getting', 'authenticated', ':', '\\n\\n', 'and', 'here', 'is', 'the', 'code', 'for', 'the', 'main.py', 'endpoint', ':', 'yes', 'ir', 'does', 'not', 'have', 'content', 'type', 'how', 'can', 'I', 'make', 'passport', 'pass', 'in', 'the', 'content', 'type', ':', 'the', 'content', 'type', 'seems', 'like', 'an', 'important', 'part', 'of', 'oauth2', ',', 'could', 'it', 'be', 'that', 'I', 'am', 'using', 'the', 'wrong', 'strategy', 'I', 'do', 'nt', 'have', 'control', 'over', 'the', '3rd', 'party', 'apps', 'requested', 'authorization_content_type', ':', '\\n  ', '\"', 'show', 'me', 'how', 'I', 'could', 'keep', 'the', 'content', 'type', 'dynamic', 'for', 'OAuth2Strategy', 'I', 'am', 'getting', 'the', 'following', 'error', ':', '\\n', 'Property', \"'\", '_', 'request', \"'\", 'is', 'protected', 'and', 'only', 'accessible', 'within', 'class', \"'\", 'OAuth2', \"'\", 'and', 'its', 'subclasses.ts(2445', ')', '\\n', '(', 'method', ')', 'OAuth2._request(method', ':', 'string', ',', 'url', ':', 'string', ',', 'headers', ':', 'OutgoingHttpHeaders', '|', 'null', ',', 'post_body', ':', 'any', ',', 'access_token', ':', 'string', '|', 'null', ',', 'callback', ':', 'dataCallback', '):', 'void', 'Lets', 'take', 'a', 'step', 'back', 'I', 'am', 'following', 'the', 'documentation', 'for', 'https://www.passportjs.org/packages/passport-oauth2/', 'as', 'I', 'want', 'to', 'replicate', 'the', 'authentication', 'documented', 'here', 'https://platform.openai.com/docs/plugins/authentication', '\\n\\n', 'considering', 'that', 'I', 'want', 'to', 'replicate', 'the', 'oauth', 'solution', 'in', 'two', 'nextjs', 'typescript', 'files', 'C:\\\\Projects\\\\OpenPlugin\\\\openplugin', '-', 'io\\\\src\\\\pages\\\\api\\\\passport.ts', 'and', 'C:\\\\Projects\\\\OpenPlugin\\\\openplugin', '-', 'io\\\\src\\\\pages\\\\api\\\\callback.ts', 'how', 'do', 'you', 'recommend', 'I', 'go', 'about', 'doing', 'it', 'here', 'is', 'the', 'type', 'for', 'OAuth2Strategy', 'options', ':', '\\n    \\n\\n', 'what', 'does', 'the', 'proxy', 'do', 'and', 'can', 'I', 'use', 'it', 'to', 'handle', 'the', 'tokenURL', 'exchange', '?', 'Auf', 'der', 'Seite', 'www.erler-zimmer.de', 'haben', 'wir', 'das', 'gleiche', 'Problem', '.', 'Auch', 'hier', 'werden', 'Kategorien', 'nicht', 'gesraped', '.', 'Hier', 'unser', 'aktueller', 'Code', ':', 'ich', 'teste', 'mal', '.']\n",
      "--------------------------------------------------\n",
      "['The', 'the', 'string', 'must', 'be', '8', 'consecutive', 'words', '.', 'Does', 'the', 'above', 'code', 'ensure', 'that', '?', 'I', 'need', 'to', 'print', 'the', '$', 'quotedString', 'too', ',', 'and', 'at', 'the', 'end', 'of', 'the', 'run', 'calculate', 'and', 'print', 'the', 'percentage', 'that', 'matched']\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "for row in df['tokens'].iloc[n:n+10]:\n",
    "    print(row)\n",
    "    print('-' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94d62efb-3a8d-4ce6-a464-728772a0905a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      Un java if I have a text block with 3 variable...\n",
      "1      Analyze this repo again : https://github.com/A...\n",
      "2           any mistakes in this : Ok what can you see ?\n",
      "3      wow .   3 sounds awesome !   Let 's discuss st...\n",
      "4                      give me a post with lots of words\n",
      "                             ...                        \n",
      "255              Is this Pressable API useful for this ?\n",
      "256    and vite config to check if it has anything to...\n",
      "257    how does omegle which uses webrtc detect if so...\n",
      "258    should that OPENAI_CODE change on a per user b...\n",
      "259    I am using the package react - native - image ...\n",
      "Name: tokens_str, Length: 260, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Convert the tokens back into string format for TF-IDF vectorizer\n",
    "df['tokens_str'] = df['tokens'].apply(lambda tokens: ' '.join(tokens))\n",
    "\n",
    "# Initialize TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(3, 4), max_features=100)  # (1, 2) for unigrams and bigrams\n",
    "\n",
    "# Fit and transform the prompts into a TF-IDF matrix\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df['tokens_str'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab18596e-c625-410e-9746-fe47630fe266",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Apply KMeans clustering (set n_clusters=2 to separate questions and commands)\n",
    "k = 5\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "df['cluster'] = kmeans.fit_predict(X_tfidf)\n",
    "\n",
    "# Optionally: Add the cluster labels to the dataframe for further inspection\n",
    "# df_merged['cluster_label'] = df_merged['cluster'].map({0: 'question', 1: 'statement'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f62f238b-a2ae-428e-a8c8-47e472bb144d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 top words: what would be, what would be the, would be the, have access to, can you write, each of the, be able to, none of the, in order to, is it possible to\n",
      "Cluster 1 top words: is it possible to, is it possible, it possible to, each of the, how to get, the best way, what would be the, do not have, have access to, what would be\n",
      "Cluster 2 top words: https github com, what are the, what is the, show me how, step by step, the number of, do want to, so that it, be able to, how to run\n",
      "Cluster 3 top words: what does this, does this mean, you show me, is it possible to, how to get, how to run, https github com, https github com antonosika, if it is, if you are\n",
      "Cluster 4 top words: in order to, if you are, on the other, does this mean, it would be, would be the, you show me, is it possible to, how to run, https github com\n"
     ]
    }
   ],
   "source": [
    "# Get the feature names (words) corresponding to each feature in the TF-IDF matrix\n",
    "feature_names = np.array(tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Find the top words per cluster\n",
    "n_top_words = 10\n",
    "for i in range(k):\n",
    "    top_indices = kmeans.cluster_centers_[i].argsort()[-n_top_words:][::-1]\n",
    "    top_words = feature_names[top_indices]\n",
    "    print(f\"Cluster {i} top words: {', '.join(top_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "6becbbc5-bf09-4830-b146-0871038c5458",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# from sentence_transformers import SentenceTransformer\n",
    "# from sklearn.cluster import KMeans\n",
    "# import numpy as np\n",
    "\n",
    "# # Load Sentence-BERT model\n",
    "# model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# # Get embeddings for each conversation prompt\n",
    "# embeddings = model.encode(df['difference'].tolist())\n",
    "\n",
    "# # Perform KMeans clustering based on the embeddings\n",
    "# k = 5\n",
    "# kmeans = KMeans(n_clusters=k, random_state=42)  # Adjust the number of clusters based on your data\n",
    "# kmeans.fit(embeddings)\n",
    "\n",
    "# # Add cluster labels to the dataframe\n",
    "# df['cluster_label'] = kmeans.labels_\n",
    "\n",
    "# # Display the first 5 sentences for each cluster\n",
    "# for i in range(k):  # Assuming you want two clusters: questions vs instructions\n",
    "#     print(f\"Cluster {i}:\")\n",
    "#     cluster_samples = df[df['cluster_label'] == i]['difference'].head(5)  # Get first 5 sentences of this cluster\n",
    "#     for sentence in cluster_samples:\n",
    "#         print(\"-\" * 50)\n",
    "#         print(sentence)\n",
    "#     # print(cluster_samples.tolist())  # Print the sentences as a list\n",
    "#     print(\"=\" * 50)  # Add separator between clusters for better readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "a716b231-1948-4ae5-bbfc-b14d6dc9a4e9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Incompatible dimension for X and Y matrices: X.shape[1] == 384 while Y.shape[1] == 100",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[236], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m cluster_centers \u001b[38;5;241m=\u001b[39m kmeans\u001b[38;5;241m.\u001b[39mcluster_centers_\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Calculate cosine similarity between each sentence's embedding and each cluster center\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m cosine_similarities \u001b[38;5;241m=\u001b[39m \u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcluster_centers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# For each cluster, find the top 5 most similar sentences to the cluster center\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(k):  \u001b[38;5;66;03m# Assuming 2 clusters\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.10/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    214\u001b[0m         )\n\u001b[1;32m    215\u001b[0m     ):\n\u001b[0;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    226\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.10/lib/python3.11/site-packages/sklearn/metrics/pairwise.py:1741\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[0;34m(X, Y, dense_output)\u001b[0m\n\u001b[1;32m   1695\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute cosine similarity between samples in X and Y.\u001b[39;00m\n\u001b[1;32m   1696\u001b[0m \n\u001b[1;32m   1697\u001b[0m \u001b[38;5;124;03mCosine similarity, or the cosine kernel, computes similarity as the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1737\u001b[0m \u001b[38;5;124;03m       [0.57..., 0.81...]])\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1739\u001b[0m \u001b[38;5;66;03m# to avoid recursive import\u001b[39;00m\n\u001b[0;32m-> 1741\u001b[0m X, Y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_pairwise_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1743\u001b[0m X_normalized \u001b[38;5;241m=\u001b[39m normalize(X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m Y:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.10/lib/python3.11/site-packages/sklearn/metrics/pairwise.py:229\u001b[0m, in \u001b[0;36mcheck_pairwise_arrays\u001b[0;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, ensure_all_finite, ensure_2d, copy)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    222\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecomputed metric requires shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    223\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(n_queries, n_indexed). Got (\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    224\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m indexed.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    225\u001b[0m         )\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m ensure_2d \u001b[38;5;129;01mand\u001b[39;00m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# Only check the number of features if 2d arrays are enforced. Otherwise,\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# validation is left to the user for custom metrics.\u001b[39;00m\n\u001b[0;32m--> 229\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    230\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncompatible dimension for X and Y matrices: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX.shape[1] == \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m while Y.shape[1] == \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    232\u001b[0m     )\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, Y\n",
      "\u001b[0;31mValueError\u001b[0m: Incompatible dimension for X and Y matrices: X.shape[1] == 384 while Y.shape[1] == 100"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Get the cluster centers\n",
    "cluster_centers = kmeans.cluster_centers_\n",
    "\n",
    "# Calculate cosine similarity between each sentence's embedding and each cluster center\n",
    "cosine_similarities = cosine_similarity(embeddings, cluster_centers)\n",
    "\n",
    "# For each cluster, find the top 5 most similar sentences to the cluster center\n",
    "for i in range(k):  # Assuming 2 clusters\n",
    "    print(f\"Cluster {i} top 5 most similar sentences: \\n\")\n",
    "    \n",
    "    # Get the indices of the sentences sorted by similarity to the cluster center (highest similarity first)\n",
    "    top_indices = np.argsort(cosine_similarities[:, i])[::-1][:5]\n",
    "    \n",
    "    # Display the top 5 most similar sentences\n",
    "    top_sentences = df.iloc[top_indices]['difference']\n",
    "    for idx, sentence in zip(top_indices, top_sentences):\n",
    "        print(f\"Similarity: {cosine_similarities[idx, i]:.4f} - {sentence}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    print(\"=\" * 50)  # Separator for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369850e4-1eaf-4f09-b104-bca6d5ee9896",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (new_base)",
   "language": "python",
   "name": "new_base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
