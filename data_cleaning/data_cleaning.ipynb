{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import json_normalize\n",
    "import numpy as np\n",
    "from langdetect import detect, LangDetectException\n",
    "from langdetect.detector_factory import DetectorFactory\n",
    "\n",
    "# Set a fixed seed for reproducibility\n",
    "DetectorFactory.seed = 108\n",
    "\n",
    "def detect_language(prompt):\n",
    "    \"\"\"Detects the language of a given text, handling exceptions.\"\"\"\n",
    "    try:\n",
    "        return detect(str(prompt)) if isinstance(prompt, str) else None\n",
    "    except LangDetectException:\n",
    "        return None\n",
    "\n",
    "\n",
    "def load_and_normalize_json(file_path):\n",
    "    \"\"\"Loads a JSON file and normalizes its 'Sources' data.\"\"\"\n",
    "    df = pd.read_json(file_path)\n",
    "    df = json_normalize(df['Sources'])\n",
    "    print(f\"Initial data rows: {len(df)}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_and_transform_data(df):\n",
    "    \"\"\"Cleans and renames columns in the dataset.\"\"\"\n",
    "    columns_to_drop = ['Type', 'Author']\n",
    "    df.drop(columns=columns_to_drop, errors='ignore', inplace=True)\n",
    "    \n",
    "    df.rename(columns={\n",
    "        \"Title\": \"issueTitle\",\n",
    "        \"URL\": \"sourceURL\",\n",
    "        \"Body\": \"issueDesc\"\n",
    "    }, inplace=True)\n",
    "    \n",
    "    df['ChatgptSharing'] = df['ChatgptSharing'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "    return df\n",
    "\n",
    "\n",
    "def normalize_and_explode_chatgpt(df):\n",
    "    \"\"\"Explodes the 'ChatgptSharing' column and propagates relevant columns.\"\"\"\n",
    "    chatgpt_sharing = json_normalize(df['ChatgptSharing'].explode())\n",
    "    print(f\"After exploding 'ChatgptSharing': {len(chatgpt_sharing)} rows\")\n",
    "    \n",
    "    if 'Title' in chatgpt_sharing.columns and 'Mention.MentionedURL' in chatgpt_sharing.columns:\n",
    "        chatgpt_sharing['conversation_id'] = chatgpt_sharing['Title'].astype(str) + '_' + chatgpt_sharing['Mention.MentionedURL'].astype(str)\n",
    "    else:\n",
    "        chatgpt_sharing['conversation_id'] = \"UNKNOWN\"\n",
    "        \n",
    "    print(f\"Unique conversation IDs: {chatgpt_sharing['conversation_id'].nunique()}\")\n",
    "    \n",
    "    # Propagate columns from the parent DataFrame\n",
    "    columns_to_propagate = df.columns.difference(['ChatgptSharing'])\n",
    "    for col in columns_to_propagate:\n",
    "        chatgpt_sharing[col] = df[col].repeat(df['ChatgptSharing'].apply(len)).reset_index(drop=True)\n",
    "    \n",
    "    return chatgpt_sharing\n",
    "\n",
    "\n",
    "def clean_chatgpt_sharing(chatgpt_sharing):\n",
    "    \"\"\"Cleans and renames columns in the exploded ChatGPT sharing data.\"\"\"\n",
    "    columns_to_drop = [\n",
    "        'Status', 'DateOfConversation', 'DateOfAccess', 'TokensOfPrompts', \n",
    "        'TokensOfAnswers', 'Model', 'HTMLContent', 'URL', 'Mention.MentionedURL', \n",
    "        'Mention.MentionedAuthor'\n",
    "    ]\n",
    "    chatgpt_sharing.drop(columns=columns_to_drop, errors='ignore', inplace=True)\n",
    "    \n",
    "    chatgpt_sharing.rename(columns={\n",
    "        'Title': 'conversationTitle',\n",
    "        'NumberOfPrompts': 'numPrompts',\n",
    "        'Mention.MentionedProperty': 'mentionProperty',\n",
    "        'Mention.MentionedText': 'mentionText'\n",
    "    }, inplace=True)\n",
    "    \n",
    "    chatgpt_sharing['Conversations'] = chatgpt_sharing['Conversations'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "    return chatgpt_sharing\n",
    "\n",
    "\n",
    "def normalize_and_explode_conversations(chatgpt_sharing):\n",
    "    \"\"\"Explodes the 'Conversations' column and propagates relevant columns.\"\"\"\n",
    "    conversations = json_normalize(chatgpt_sharing['Conversations'].explode())\n",
    "    print(f\"After exploding 'Conversations': {len(conversations)} rows\")\n",
    "    \n",
    "    if 'conversation_id' not in conversations.columns:\n",
    "        conversations['conversation_id'] = chatgpt_sharing['conversation_id'].repeat(chatgpt_sharing['Conversations'].apply(len)).reset_index(drop=True)\n",
    "\n",
    "    columns_to_propagate = chatgpt_sharing.columns.difference(['Conversations'])\n",
    "    for col in columns_to_propagate:\n",
    "        conversations[col] = chatgpt_sharing[col].repeat(chatgpt_sharing['Conversations'].apply(len)).reset_index(drop=True)\n",
    "    \n",
    "    return conversations\n",
    "\n",
    "\n",
    "def filter_english_conversations(conversations):\n",
    "    \"\"\"Filters conversations to keep only those in English.\"\"\"\n",
    "    conversations = conversations.copy()\n",
    "    conversations = conversations[~(conversations['Prompt'].isnull() | (conversations['Prompt'].str.strip() == ''))]\n",
    "    conversations['Detected_Language'] = conversations['Prompt'].apply(detect_language)\n",
    "    print(f\"Language detection applied. Null languages: {conversations['Detected_Language'].isnull().sum()}\")\n",
    "    \n",
    "    if 'conversation_id' in conversations.columns:\n",
    "        mode_languages = conversations.groupby('conversation_id')['Detected_Language'].agg(lambda x: x.mode()[0] if not x.mode().empty else None)\n",
    "        english_conversations = mode_languages[mode_languages == 'en'].index.tolist()\n",
    "        conversations = conversations[conversations['conversation_id'].isin(english_conversations)]\n",
    "        print(f\"English conversations: {len(conversations)} rows, {conversations['conversation_id'].nunique()} unique IDs\")\n",
    "    else:\n",
    "        print(\"Error: 'conversation_id' is missing from conversations dataset.\")\n",
    "    \n",
    "    return conversations\n",
    "\n",
    "\n",
    "def process_json(file_path):\n",
    "    \"\"\"Executes the entire processing pipeline for a given JSON file.\"\"\"\n",
    "    df = load_and_normalize_json(file_path)\n",
    "    df = clean_and_transform_data(df)\n",
    "    chatgpt_sharing = normalize_and_explode_chatgpt(df)\n",
    "    chatgpt_sharing = clean_chatgpt_sharing(chatgpt_sharing)\n",
    "    conversations = normalize_and_explode_conversations(chatgpt_sharing)\n",
    "    conversations = filter_english_conversations(conversations)\n",
    "    \n",
    "    return conversations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing issue data...\n",
      "Initial data rows: 353\n",
      "After exploding 'ChatgptSharing': 417 rows\n",
      "Unique conversation IDs: 406\n",
      "After exploding 'Conversations': 1780 rows\n",
      "Language detection applied. Null languages: 1\n",
      "English conversations: 1431 rows, 313 unique IDs\n",
      "\n",
      "Processing discussion data...\n",
      "Initial data rows: 45\n",
      "After exploding 'ChatgptSharing': 51 rows\n",
      "Unique conversation IDs: 51\n",
      "After exploding 'Conversations': 192 rows\n",
      "Language detection applied. Null languages: 0\n",
      "English conversations: 154 rows, 41 unique IDs\n"
     ]
    }
   ],
   "source": [
    "# Clean issue and discussion data\n",
    "\n",
    "## Issue data\n",
    "file_path = '../data/snapshot_20230831/20230831_061759_issue_sharings.json'\n",
    "\n",
    "print('Processing issue data...')\n",
    "processed_conversations_issue = process_json(file_path)\n",
    "\n",
    "processed_conversations_issue.to_csv('../data/data_clean/issuedata_cleaned.csv', index=False)\n",
    "\n",
    "## Discussion data\n",
    "file_path = '../data/snapshot_20230831/20230831_061926_discussion_sharings.json'\n",
    "\n",
    "print('\\nProcessing discussion data...')\n",
    "processed_conversations_disc = process_json(file_path)\n",
    "\n",
    "processed_conversations_disc.to_csv('../data/data_clean/discussiondata_cleaned.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
